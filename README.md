# Deep Learning Study Notes üß†

This is a comprehensive collection of study notes on fundamental deep learning literature, focusing on practical implementation and theoretical foundations.
> ‚ö†Ô∏è All original notes are in Chinese

## üìö Included Literature

### Building Large Language Models from Scratch
- **Author**: Sebastian Raschka
- **Publisher**: Manning Publications (2025)
- **ISBN**: 9781633437166

**Core Content**:
- LLM architecture design
- Text processing and tokenization
- Attention mechanism implementation
- GPT model construction
- Self-supervised pre-training
- Classification task fine-tuning
- Instruction fine-tuning

### Transformer Models for NLP and Computer Vision
- **Author**: Denis Rothman
- **Publisher**: Packt Publishing Limited (2024)
- **ISBN**: 978-1-80512-872-4

**Core Content**:
- Advanced Transformer architectures
- Vision Transformers
- Practical applications with Hugging Face
- Quality control and risk management
- Multimodal AI systems (CLIP, SAM, etc.)
- Diffusion model principles

### Deep Learning for Computer Vision
- **Author**: Mohamed Elgendy
- **Publisher**: Manning Publications (2020)
- **ISBN**: 9781617296192

**Core Content**:
- Deep learning fundamentals
- CNN architecture and implementation
- Hyperparameter optimization
- Transfer learning applications
- Advanced CNN architectures
- Generative Adversarial Networks

### OpenManus Framework Analysis
**Core Content**:
- Agent system architecture design
- Tool system design and implementation
- ReAct pattern in practice
- Browser automation tools
- File operations and terminal tools

## üìù Note Structure
- Dedicated directory for each book
- Complete derivations and code implementations
- Organized screenshots of source materials
- Practical examples and hands-on exercises

## üìÑ License
This project is licensed under the MIT License. You can:
- Fork this repository
- Copy specific book notes
- Use code under license terms

## ‚ö†Ô∏è Disclaimer
All image content is from the original books and used for educational purposes only. If there are any copyright concerns, please contact us.