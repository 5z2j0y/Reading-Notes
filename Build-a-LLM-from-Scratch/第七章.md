微调来接收指令
---

![alt text](images/build-a-llm/image_rename59.png)
目标是从prompt+input，从prompt的第一个词开始一个一个token地预测，最后提取出output(自回归)

让模型“听懂”指令，然后执行指令

![alt text](images/build-a-llm/image_rename54.png)
*训练指令微调的三个步骤：准备数据，微调模型，评估模型*

### 阶段一：准备指令数据集
#### 1. 数据集
由instruction + input -> output组成


```
Example entry: 
{'instruction': "What is an antonym of 'complicated'?", 
'input': '', 
'output': "An antonym of 'complicated' is 'simple'."}
```
*数据长这样。input也可以为空*

##### 转换Prompt风格（预处理）
![alt text](images/build-a-llm/image_rename55.png)
为了适配已有的微调框架，让推理效果更稳定，我们会转换成特定Prompt Style例如Alpaca或者Phi-3

##### 划分数据集

#### 2. 数据加载器
##### 自制数据加载器
![alt text](images/build-a-llm/image_rename56.png)
*步骤*：
改用Prompt模板 → 分词  → 把句子Padding到相同长度 → 创建目标向量 → 对齐

![alt text](images/build-a-llm/image_rename57.png)
*在计算损失时，通过掩膜把指令和输入的token id 设置为-100，只会统计输出的损失*

> 对于在指令微调过程中屏蔽指令是否普遍有益，还存在分歧

#### 3. 利用数据加载器实现批处理
(略)

### 阶段二：微调模型
#### 4. 加载预训练模型
使用"gpt2-medium (355M)"

#### 5. 在指令数据集上微调模型

### 阶段三：评估模型
![alt text](images/build-a-llm/image_rename58.png)

#### 6. 提取模型的响应
(略)

#### 7. 评估模型的性能
评价方法：
- 简答题和多选题的准确率：测试一般知识
- 人类偏好与其他LLM的比较：测试指令理解能力
- 自动会话基准：测试模型的对话能力

> 人类评估：数据宝贵，但效率低
> - 常见方法：A/B测试，量表打分
> 
> 自动评估：用代码跑标准测试集
> - 常见方法：BLEU，ROUGE，Perplexity


小结  指令微调过程可调整预训练 LLM，使其遵循人类指令并生成所需的响应。 数据集的准备工作包括下载指令-响应数据集、格式化条目并将其分成训练集、验证集和测试集。 使用自定义整理函数构建训练批次，该函数可填充序列、创建目标标记 ID 并屏蔽填充标记。 我们加载了一个带有 3.55 亿个参数的预训练 GPT-2 中型模型，作为指令微调的起点。 使用与预训练类似的训练循环，在指令数据集上对预训练模型进行微调。 评估包括在测试集上提取模型响应并对其进行评分（例如，使用另一个 LLM）。 带有 80 亿参数 Llama 模型的 Ollama 应用程序可用于自动对微调模型在测试集上的响应进行评分，提供平均分以量化性能。

---

## 参考资源

### 学习资源
- 机器学习最新论文：[arxiv.org/list/cs.LG/recent](https://arxiv.org/list/cs.LG/recent)
  > 及时了解机器学习领域的前沿研究动态
- 作者个人博客：[sebastianraschka.com/blog](https://sebastianraschka.com/blog/)
  > 获取作者的最新研究和教程

### 实用工具
- [Axolotl](https://github.com/axolotl-ai-cloud/axolotl)
  > 封装好的LLM后处理工具，简化模型训练流程（简单一些些）
- [LitGPT](https://github.com/Lightning-AI/litgpt)
  > 基于PyTorch Lightning的GPT实现

---
完结撒花！🎉