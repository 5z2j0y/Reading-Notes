### Transformer的背景
2018：BERT
以前：RNN、LSTM、GRU
好处：并行计算、长距离依赖
BLEU：翻译任务的评价指标

### Transformer的结构
- 输入
- 输出
- Encoder
- Decoder

#### 输入
- 词向量
- 位置向量

#### 输出
- 词向量
- SOFTMAX

#### Encoder
- N个相同的层
- 每层包含两个子层
    - 多头自注意力
    - 前馈神经网络

#### Decoder
- N个相同的层
- 每层包含三个子层
    - 多头自注意力
    - 多头注意力
    - 前馈神经网络

#### 输入：嵌入
参数：词向量维度、词表大小（padding、masking）

#### 输入：位置编码
参数：位置向量维度、最大长度 [,dropout]

- 初始化绝对位置矩阵
- 初始化位置编码矩阵
